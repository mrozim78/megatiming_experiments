{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import xmltodict\n",
    "import pymongo\n",
    "class MegatimingSpider(scrapy.Spider):\n",
    "    name = \"MegatimingSpider\"\n",
    "    errors = [];\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://megatiming.pl/contests/regional/1',\n",
    "            'http://megatiming.pl/contests/regional/2',\n",
    "            'http://megatiming.pl/contests/regional/3',\n",
    "            'http://megatiming.pl/contests/regional/4',\n",
    "            'http://megatiming.pl/contests/regional/5',\n",
    "            'http://megatiming.pl/contests/regional/6',\n",
    "            'http://megatiming.pl/contests/regional/7',\n",
    "            'http://megatiming.pl/contests/regional/8',\n",
    "            'http://megatiming.pl/contests/regional/9',\n",
    "            'http://megatiming.pl/contests/regional/10',\n",
    "            'http://megatiming.pl/contests/regional/11',\n",
    "            'http://megatiming.pl/contests/regional/12',\n",
    "            'http://megatiming.pl/contests/regional/13',\n",
    "            'http://megatiming.pl/contests/regional/14',\n",
    "            'http://megatiming.pl/contests/regional/15',\n",
    "            'http://megatiming.pl/contests/regional/16',\n",
    "            'http://megatiming.pl/contests/regional/17',\n",
    "            'http://megatiming.pl/contests/regional/18',\n",
    "            'http://megatiming.pl/contests/regional/19',\n",
    "            'http://megatiming.pl/contests/regional/20',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        js_script = response.xpath('/html/body/script[contains(., \"contests\")]/text()').get()\n",
    "        json_source=js_script[js_script.find('data=')+6:-1]       \n",
    "        json_source_1 =  json_source[json_source.find('\"contests\"')+11:] \n",
    "        json_source_2 = json_source_1[:json_source_1.find('\"home\"')-1];\n",
    "        data = json.loads(json_source_2)\n",
    "        for key in data[\"data\"][\"contests\"].keys():\n",
    "            #print(key)\n",
    "            yield scrapy.Request(url=\"http://megatiming.pl/contest/\"+key,\n",
    "                             callback=self.parseContest,\n",
    "                             cb_kwargs=dict(uuid=key))\n",
    "        \n",
    "    def parseContest(self,response,uuid):\n",
    "        fileName = self.getFileName(response,uuid)\n",
    "        if (fileName):\n",
    "            self.writeToDatabase(fileName,uuid)\n",
    "    \n",
    "    def getFileName(self,response,uuid):\n",
    "        try:\n",
    "            js_script = response.xpath('/html/body/script[contains(., \"contests\")]/text()').get()\n",
    "            json_source_1=js_script[js_script.find('\"contests\"')+11:-1]\n",
    "            json_source_2=json_source_1[:json_source_1.find('\"home\"')-1]\n",
    "            data = json.loads(json_source_2)\n",
    "            for element in data[\"data\"][\"contests\"][uuid][\"files\"][\"result\"]:\n",
    "                if (element[\"url\"].find(\"lxf\")>=0):\n",
    "                    return(element[\"url\"])\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def writeToDatabase(self,url,uuid):\n",
    "        try:\n",
    "            resp = urlopen(url)\n",
    "            zipfile = ZipFile(BytesIO(resp.read()))\n",
    "            nameFileLef = zipfile.namelist()[0]\n",
    "            xmlFile = zipfile.read(nameFileLef).decode(\"utf-8\") \n",
    "        \n",
    "            # change wrong name attributes\n",
    "            #result.url, organizer.url, hostclub.url -> result_url , organizer_url , hostclub_url\n",
    "            xmlFile = xmlFile.replace(\"result.url\",\"result_url\");\n",
    "            xmlFile = xmlFile.replace(\"organizer.url\",\"organizer_url\");\n",
    "            xmlFile = xmlFile.replace(\"hostclub.url\",\"hostclub_url\");\n",
    "            xmlFile = xmlFile.replace(\"shortname.en\",\"shortname_en\")\n",
    "            xmlFile = xmlFile.replace(\"firstname.en\",\"firstname_en\")\n",
    "            xmlFile = xmlFile.replace(\"name.en\",\"name_en\")\n",
    "            xmlFile = xmlFile.replace(\"lastname.en\",\"lastname_en\")\n",
    "            xmlFile = xmlFile.replace(\"city.en\",\"city_en\")\n",
    "        \n",
    "            # read to dictionary\n",
    "            data = xmltodict.parse(xmlFile)\n",
    "            data[\"uuid\"] = uuid\n",
    "            # open mongodb and write dictionary\n",
    "            myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "            mydb = myclient[\"megatiming\"]\n",
    "            mycol = mydb[\"results\"]\n",
    "            mycol.insert_one(data)\n",
    "       \n",
    "        except Exception as e:\n",
    "            self.errors.append(uuid+\" error \"+str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "process = CrawlerProcess()\n",
    "spider = MegatimingSpider\n",
    "process.crawl(spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider.errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
